Хабрβ Открыть список Как стать автором Моя лентаВсе потокиРазработкаАдминистрированиеДизайнМенеджментМаркетингНаучпоп Поиск Написать публикацию Настройки Войти Обновить 79.31 Рейтинг Подписаться Big Data * Большие данные и всё о них СтатьиПостыНовостиАвторыКомпании Все подряд Открыть список Скопировать ссылку на RSS Сначала показывать Новые Лучшие Порог рейтинга Все ≥0 ≥10 ≥25 ≥50 ≥100 Уровень сложности Все Простой Средний СложныйПрименить badcasedaily1 23 часа назад Промежуточные витрины в SQL Время на прочтение8 мин Количество просмотров 813 Блог компании OTUSSQL*Big Data* Обзор Привет, Хабр! Сегодня я хочу поговорить о том, без чего не обходится практически ни один серьёзный проект с большими данными (да и с не слишком большими тоже) — о промежуточных витринах (или более привычно – staging, core, data mart). Читать далее Всего голосов 2: ↑1 и ↓1 0 Добавить в закладки8 Комментарии 0 Новости все подрядлучшие Все новости kucev 27 мар в 13:40 LLM red teaming: полное руководство [+советы экспертов] Время на прочтение12 мин Количество просмотров 649 Data Mining*Искусственный интеллектМашинное обучение*Big Data*Data Engineering* Перевод Давайте представим стратегию, зародившуюся в военной сфере, где команды притворяются врагами друг друга, чтобы проверить оборонительные механизмы. Этот подход, известный как red teaming, оказался чрезвычайно ценным и теперь нашёл новое применение. Сегодня, когда искусственный интеллект занимает всё больше места в нашей повседневной жизни, использование метода red teaming для тестирования этих систем становится необходимым. Red teaming для моделей-LLM помогает убедиться, что они не только эффективны в работе, но и безопасны и надежны. Читать далее Всего голосов 2: ↑2 и ↓0 +2 Добавить в закладки7 Комментарии 0 evgeniatro 27 мар в 13:34 Как мы размечали более 800 часов аудио: от бытовых диалогов до шумных медицинских записей Уровень сложностиПростой Время на прочтение3 мин Количество просмотров 378 Блог компании Data LightBig Data* Кейс Многим может показаться, что может быть сложного в аудиоразметке? Надел наушники, включил запись — и вперед, переписывай все, что слышишь. Но, как показал этот проект, даже такая на первый взгляд стандартная задача превращается в настоящее испытание, когда дело доходит до сотен часов сложных записей с медицинских устройств и фоновым шумом. Рассказываем, как нам удалось не только качественно обработать более 800 часов аудио, но и выстроить процесс так, чтобы он оставался эффективным и прозрачным даже в самых сложных условиях. Читать далее Всего голосов 1: ↑1 и ↓0 +1 Добавить в закладки6 Комментарии 1 sag25 27 мар в 10:01 Как мы покорили методы Big Data для данных любого размера Уровень сложностиПростой Время на прочтение5 мин Количество просмотров 1.7K Блог компании МТСBig Data*Data Engineering* Кейс Всем привет! Меня зовут Саттар Гюльмамедов и я работаю в команде ETL платформы DataOps в МТС. Марк Твен как-то написал «Слухи о моей смерти сильно преувеличены» — про Big Data сейчас можно сказать то же самое. Волна хайпа, которую многие пытались оседлать, прошла. Но, как и значительная часть инженерных достижений, работа с большими данными стала рутиной, помогающей развиваться другим направлениям в ИТ. В экосистеме МТС мы строим для Big Data отдельную платформу, где есть инструменты для хранения и оценки данных, анализа и построения отчетов. Но все начинается с их загрузки и обработки. Получение и преобразование данных — как раз задача библиотек и сервисов, которые делает моя команда. Многие знают мем о перекладывании JSON. А мы как раз делаем инструменты для тех случаев, когда такие задачи уже не столь тривиальны и нужно разобраться с разными типами данных, разными структурам, хранящимися к тому же в разных форматах, и все это нужно сделать в рамках одного процесса. В этом материале я расскажу про наши решения и условия, лежащие в их основе. Одним наш опыт поможет спланировать эволюцию своих инструментов, другим снимет страх перед сложным стеком технологий Big Data, а третьи просто развлекутся. Дисклеймер: чтобы не отклоняться от темы, я не буду подробно описывать концепции ETL и ELT (они хорошо разобраны тут, тут и тут). Наши инструменты следуют парадигме «E[TL]+», т. е. позволяют выполнять трансформации данных как в процессе переноса, так и в целевом хранилище. Про нашу платформу в общих чертах писал мой коллега Дмитрий Бодин в своей публикации «Customer Happiness: как не только разработать, но и внедрить новый продукт внутри крупной компании». Я продолжу начатый им рассказ и добавлю подробностей о компоненте ETL, его составляющих и нашей команде. Читать далее Всего голосов 8: ↑7 и ↓1 +8 Добавить в закладки15 Комментарии 0 Истории shirokova_ea 26 мар в 16:51 Применение ML Pricing в ритейле: хвост виляет собакой Время на прочтение6 мин Количество просмотров 794 Блог компании Lenta TechИскусственный интеллектМашинное обучение*Алгоритмы*Big Data* Кейс Привет, Habr! Мы Катя и Оля, продакт-менеджеры BigData в компании «Лента», отвечаем за развитие цифровых продуктов блоков «Ассортимент» и «Ценообразование». В этой статье расскажем про внедрение ML-модели и алгоритма ценообразования товаров «хвоста», а также - трудности, с которыми столкнулись. Читать далее Всего голосов 6: ↑4 и ↓2 +4 Добавить в закладки11 Комментарии 2 mark-rtb 26 мар в 14:29 От скриптов к сервисам: 10 книг для профессиональной разработки в Data Science Уровень сложностиСредний Время на прочтение7 мин Количество просмотров 1.3K Блог компании Ozon БанкBig Data*Профессиональная литература*Машинное обучение*Python* Мнение Привет! Меня зовут Марк Паненко, и я Chief Data Science в Ozon Банке. Сегодня я хочу поговорить про книги, которые научат писать код. В современной экосистеме Data Science недостаточно просто знать алгоритмы машинного обучения и статистические методы — необходимы прочные инженерные навыки для создания масштабируемых, поддерживаемых решений. Это третья часть серии статей о главных книгах для data-специалистов. В первой части «От комиксов до нейросетей» я писал о литературе для джунов. Во второй — «Код устареет, принципы — останутся» — для мидлов и сеньоров. В этой же части мы сфокусируемся исключительно на книгах для развития навыков программиста, ставших необходимым для современного дата-сайентиста. Основываясь на опыте моего подкаста «Дата Завтрак», я структурировал подборку по пути профессионального роста инженера: от фундаментальных навыков до специализированных продакшн-инструментов. Читать далее Всего голосов 1: ↑1 и ↓0 +1 Добавить в закладки34 Комментарии 0 Vasilenko_Ilia 25 мар в 19:30 Сделал тг бот по подсчету калорий за 2 часа и похудел на 10кг Уровень сложностиСредний Время на прочтение3 мин Количество просмотров 3K Big Data*Здоровье Кейс От идеи до работающего Telegram бота за 2 часа, от 112 кг до 102 кг за 2 месяца. Это история о том, как использование Cursor, v0.dev и современных AI-инструментов помогает решать личные проблемы с помощью кода — и как это личное решение превращается в бизнес-возможность. Читать далее Всего голосов 16: ↑3 и ↓13 -8 Добавить в закладки18 Комментарии 14 alizar 25 мар в 12:01 Картель влиятельных датасетов в обучении ИИ Уровень сложностиПростой Время на прочтение8 мин Количество просмотров 2.7K Блог компании RUVDS.comBig Data*Искусственный интеллектМашинное обучение*Научно-популярное В последнее время такие компании, как OpenAI и Google, заключают эксклюзивные соглашения об обмене данными с издателями, крупными форумами вроде Reddit и социальными медиаплатформами, что становится для них ещё одним способом сконцентрировать власть. Такая тенденция выгодна крупнейшим игрокам на рынке ИИ, которые могут позволить себе такие сделки, в отличие от исследователей, некоммерческих организаций и небольших компаний. Крупнейшие корпорации располагают лучшими ресурсами для сбора датасетов. В результате эталонные датасеты для бенчмарков (и для обучения) моделей ИИ всё больше концентрируются в руках малого количества корпораций и академических учреждений. По мнению некоторых исследователей, это «новая волна асимметричного доступа», которой ещё не было в истории открытого интернета в таком масштабе. Читать дальше → Всего голосов 23: ↑23 и ↓0 +36 Добавить в закладки24 Комментарии 0 aleksei_terentev 25 мар в 01:41 Propensity Score Matching (PSM): как обойтись без A/B-теста и всё равно узнать правду Уровень сложностиСредний Время на прочтение9 мин Количество просмотров 886 Машинное обучение*Big Data*Data Mining*Искусственный интеллектСтатистика в IT Как определить, влияет ли то или иное событие на ключевые метрики, если полноценный A/B-тест недоступен? В этой статье мы разберём метод Propensity Score Matching (PSM): узнаем, как компенсировать отсутствие рандомизации, выровнять группы по ключевым признакам и избежать ложных выводов при оценке эффектов. Читать далее Всего голосов 6: ↑6 и ↓0 +7 Добавить в закладки22 Комментарии 0 Mostransproekt 25 мар в 00:15 Не окей, гугл: как сделать поисковик для работы с служебными презентациями Уровень сложностиСредний Время на прочтение11 мин Количество просмотров 940 Блог компании МосТрансПроектИскусственный интеллектХранение данных*Big Data* Кейс Привет, Хабр! Это снова команда «МосТрансПроекта». Мы постоянно работаем с информацией и знаниями, которые храним в служебных презентациях. Чтобы ими было удобней пользоваться и извлекать данные, мы решили создать удобный сервис хранения документов с поиском. Задача оказалась непростой, и в этой статье мы расскажем, как её решили. Текст будет интересен всем, кто занимается структурированием данных, поисковыми машинами и ИИ. Читать далее Всего голосов 2: ↑2 и ↓0 +2 Добавить в закладки7 Комментарии 0 leadVSK 21 мар в 15:01 Тестирование платформы DeepSeek для проверки гипотез по анализу данных Уровень сложностиПростой Время на прочтение3 мин Количество просмотров 4.8K Блог компании Страховой Дом ВСКМашинное обучение*Тестирование IT-систем*Big Data*Искусственный интеллект Обзор Привет, Хабр! Мы, ребята из Центра эксплуатации Блока ИТ Страхового Дома ВСК, занимаемся управлением автоматизации ИТ-процессов. И у нас, как у всех — куча прикладных задач, которые хочется закрыть быстро дешево и качественно. Недавний хайп по Deepseek не обошел нас стороной, и мы решили протестировать платформу по парочке гипотез в надежде на чудо. И так, мы решили сфокусироваться на потребностях нашей команды технической поддержки в части анализа и обработки данных по ключевым метрикам и категоризации обращений. Гипотеза 1: Оценка тенденций ключевых показателей технической поддержки Мы решили проверить, насколько DeepSeek способен анализировать динамику показателей. В качестве данных взяли выгрузку по основным метрикам техподдержки: SLA, количество заявок (поступило/решено), количество негативных отзывов и пр. Скармливали выгрузку Excel, в общем то, простая таблица со следующими показателями (столбцы): Читать далее Всего голосов 13: ↑13 и ↓0 +14 Добавить в закладки42 Комментарии 2 koanse 21 мар в 13:43 Изучаем DAX Time Intelligence с помощью ViTalk GPT Время на прочтение3 мин Количество просмотров 1K Блог компании VisiologyBig Data*Визуализация данных*Искусственный интеллект Кейс Привет, Хабр! Сегодня я хочу поговорить о возможностях и ограничениях функций Time Intelligence в Visiology. Это очень интересный раздел языка DAX, который позволяет быстро делать показательные расчеты, например, сравнивая показатели текущего периода с предыдущими. Однако в его реализации для Visiology и Power BI есть некоторые различия (впрочем, не влияющие на результат). В этой статье мы поговорим об этой разнице, а также я наглядно покажу, как чат-бот ViTalk GPT помогает разобраться с особенностями работы различных функций. Читать далее Всего голосов 4: ↑4 и ↓0 +6 Добавить в закладки2 Комментарии 0 kirillsergeev0102 20 мар в 15:27 Дедупликация объявлений: как мы боремся с одинаковыми размещениями Уровень сложностиСложный Время на прочтение13 мин Количество просмотров 1.6K Блог компании ЦианМашинное обучение*Алгоритмы*Data Engineering*Big Data* Туториал Привет! Меня зовут Кирилл Сергеев, я ML-инженер в Циане. В этой статье я расскажу, как мы решили задачу дедупликации объявлений о недвижимости, разработав систему на основе трёх моделей. Эта система автоматически находит и объединяет дублирующиеся объявления, помогая пользователям видеть только актуальную и уникальную информацию. Материал будет полезен ML-инженерам и специалистам по обработке данных, которым интересно, как мы подошли к решению этой задачи: какие методы использовали, какие проблемы возникли и как мы их преодолели. Читать далее Всего голосов 10: ↑10 и ↓0 +12 Добавить в закладки27 Комментарии 5 Ближайшие события 31 марта Серия вебинаров «DevOps Middle: AvitoTech vs ecom.tech» Онлайн Больше событий в календаре Разработка Администрирование 3 апреля Реалити для разработчиков: узнайте, как строится новое публичное облако MWS Онлайн Больше событий в календаре Разработка 3 апреля Открытая встреча «System Analysis Meetup SberHealth» Москва • Онлайн Больше событий в календаре Аналитика 4 – 5 апреля Геймтон «DatsCity» Онлайн Больше событий в календаре Разработка 8 апреля Конференция TEAMLY WORK MANAGEMENT 2025 Москва • Онлайн Больше событий в календаре Менеджмент Другое 10 апреля «GoCloud 2025» — масштабная IT-конференция про облака и AI Москва • Онлайн Больше событий в календаре Разработка Администрирование Менеджмент 15 – 16 апреля Форум «Российская неделя ЦОД»: конференции «В Облаке.РФ» и TechDay Москва • Онлайн Больше событий в календаре Разработка Менеджмент Другое 17 – 19 апреля Курс: «Клиентские данные в энтерпрайзе» Москва Больше событий в календаре Аналитика Тестирование Другое 25 – 26 апреля IT-конференция Merge Tatarstan 2025 Казань Больше событий в календаре Разработка Маркетинг Другое 25 апреля Кейс-конференция «Клиника на миллиард: лидеры делятся опытом построения клиник» Москва • Онлайн Больше событий в календаре Менеджмент Другое 20 – 22 июня Летняя айти-тусовка Summer Merge Ульяновская область Больше событий в календаре Разработка Другое Влево Вправо evgeniatro 20 мар в 13:25 Группировка объявлений в карточки: как мы разметили 20 000 товаров Уровень сложностиПростой Время на прочтение2 мин Количество просмотров 474 Блог компании Data LightBig Data*Машинное обучение* Кейс Казалось бы, стандартная задача: взять 20 000 объявлений, определить в них модель товара и сгруппировать по карточкам – легкий проект, который можно закрыть за пару месяцев. Но на деле все усложняют многоязычные названия, аббревиатуры, субъективные решения аннотаторов и нюансы классификации. Как мы выстроили процесс, чтобы обеспечить точность группировки, как мы валидировали данные и какие решения помогли нам справиться с вызовами? Рассказываем в этой статье. Читать далее Всего голосов 4: ↑3 и ↓1 +2 Добавить в закладки5 Комментарии 1 GeorgeNordic 19 мар в 14:26 Что подразумевают под Data Governance? Уровень сложностиСредний Время на прочтение2 мин Количество просмотров 1.4K Big Data*Data Engineering*Терминология ITХранение данных* Если говорить про Data Governance, то это, в первую очередь, не продукты, а огромная методология управления жизненным циклом данных, и только потом – технологии. Близко к идеалу считается методология DAMA-DMBOK, и у любого специалиста по данным это должна быть настольная книга. К сожалению, в подавляющем большинстве случаев, когда люди начинают задумываться про управление данных, она попросту неприменима, так как она показывает «правильное» управление данными больших предприятий, до неё еще надо «дорасти», при этом точечно применяя сначала простые приемы, с возможностью расширения методик управления данными как «вширь», на другие отделы, так в «вглубь» на все процессы, связанные с управлением данными (Data Management): получением («добычей»), обработкой, хранением, извлечением и использованием информации. Без подобного управления жизненным циклом данных получим картину как в последнем исследовании Makves, что 40% данных никогда не используется: к ним не зафиксировано ни одного обращения за 5 лет. Найти «Ценность в данных» становится искусством, так как на предприятии растут «Кладбища данных» вместо «Хранилищ данных». Сейчас зачастую под Data Governance имеют в виду две части, это Data Quality – управление качеством данных, и Data Linage – «понять, откуда пришли данные, как они изменялись и можно ли им доверять». Если данные методологии использовать «в лоб», то это очень сильно замедлит разработку и перегрузит команду по управлению данными. Читать далее Всего голосов 2: ↑1 и ↓1 0 Добавить в закладки14 Комментарии 0 prfnv 18 мар в 16:37 Бьем автоматизацией по ручной работе с данными: как мы избавились от рутины с ML-моделями Уровень сложностиПростой Время на прочтение7 мин Количество просмотров 5.6K Блог компании МТСМашинное обучение*Data Mining*Big Data* Кейс Всем привет! Это DS-ы Павел Парфенов и Максим Шаланкин из команды Финтеха Big Data МТС. Мы и наши коллеги Data Scientists и Data Analysts ежедневно обрабатываем огромные массивы информации, строим модели и выделяем целевые сегменты, чтобы принимать обоснованные решения. Наши рутинные задачи — предварительный анализ данных (EDA), обучение ML-моделей и сегментация аудитории — часто отнимают кучу времени и ресурсов. Для себя и коллег с другими компетенциями мы решили сделать инструмент, который сэкономит время на рутинных задачах. В этой публикации мы подробно расскажем, что именно оптимизировали с помощью автоматизации и на каких этапах рабочего процесса применяем нашу командную платформу. Используя этот опыт, вы сможете освободиться от монотонных действий при работе с данными и сосредоточиться на по-настоящему важных вещах. Читать далее Всего голосов 30: ↑29 и ↓1 +30 Добавить в закладки53 Комментарии 4 DenisBerezutskiy 18 мар в 13:13 «Ошибка на миллиард» в ИИ: боремся с галлюцинациями в LLM по аналогии с NullPointerException Уровень сложностиСредний Время на прочтение20 мин Количество просмотров 3.6K Блог компании YADROИскусственный интеллектМашинное обучение*Big Data* Привет! Меня зовут Денис Березуцкий, я старший инженер по разработке ПО искусственного интеллекта в YADRO. В ML-команде мы разрабатываем системы, которые облегчают работу нашим заказчикам с помощью текстовых генеративных нейросетей: реализуем RAG, создаем чат-ботов, агентные системы и другие решения. Как и многие в индустрии, мы сталкиваемся с проблемами галлюцинаций LLM, которые портят ответы виртуальным ассистентам и способны подорвать доверие к ним. В статье я расскажу об одном не совсем стандартном методе, перенесенном из «классического» программирования, который мы применяем для борьбы с галлюцинациями и улучшения поисковой выдачи. Читать далее Всего голосов 18: ↑18 и ↓0 +20 Добавить в закладки40 Комментарии 8 Vital18 18 мар в 03:57 Программный код в Big data и Power law Уровень сложностиСредний Время на прочтение6 мин Количество просмотров 804 Big Data*Data Mining*Python*Алгоритмы*Криптография* Аналитика В статье приводятся оригинальные модули Python и даётся пояснение по их применению в задачах распределённой децентрализованной сети по типу блокчейн или, другими словами, в процессах самоорганизованной критичности (SOC). В научных публикациях чаще встречается физический термин SOC в качестве концепции, включающей процессы турбулентности, детонации, землетрясения, нейросети, фондовая волатильность, социальный рейтинг и другие. Для процессов SOC характерно отсутствие управляющих параметров и масштабная инвариантность. Универсальность сложных процессов SOC со степенным законом Power law имеет тот же характер, как и универсальность простых линейных систем, не обладающих масштабной инвариантностью, по отношению к закону нормального распределения вероятности. Зависимость от масштаба возникает при аналого-цифровом преобразовании битов в позиционную систему счисления и проявляется в законе нормального распределения вероятности в виде дисперсии и математического ожидания. Потеря масштабной инвариантности в позиционной системе счисления компенсируется приобретением принципа причинности. Например, в Древнем Риме, где была принята непозиционная система счисления, вычисляли, что «после того - не вследствие того» и сильно удивились бы истории с падающим на Ньютона яблоком. Значительные достижения в анализе Big data заставляют предположить связь с распределением вероятности Пуассона: чем больше данных, тем чаще должны встречаться пуассоновские события и вопрос лишь в поиске подходящей метрики и системы счисления. Читать далее Всего голосов 1: ↑0 и ↓1 -1 Добавить в закладки7 Комментарии 7 vstorozhilov 17 мар в 23:40 Три необсуждаемых вопроса о параллельной распределённой обработке данных — чтобы жить стало легче Уровень сложностиСредний Время на прочтение12 мин Количество просмотров 2K Data Engineering*Распределённые системы*Big Data*Hadoop*Микросервисы* Туториал Воркшоп для тех, кто впервые сталкивается с распределёнными системами. В этой статье на примере решения несложного архитектурного кейса я покажу, что ответов только на 3 вопроса при проектировании систем распределённой параллельной обработки данных будет достаточно для обеспечения жёстких нефункциональных требований. Читать далее Всего голосов 3: ↑3 и ↓0 +3 Добавить в закладки31 Комментарии 0 alfablend 17 мар в 21:12 Как мы искали должников при помощи Pandas Уровень сложностиСредний Время на прочтение6 мин Количество просмотров 9K Python*Big Data*УрбанизмОткрытые данные*SQLite* Туториал Петербургский Фонд капитального ремонта опубликовал документы, в которых указана задолженность за каждую квартиру в городе по итогам 2024 года. Мы изучили эти файлы, чтобы ответить на вопрос: где и почему хуже всего платят за ремонт в своём доме. Я занимаюсь анализом данных и дата‑журналистикой в газете «Деловой Петербург». Расскажу о том, как объединяли информацию из множества локальных html‑таблиц и приведу примеры кода на «Питоне». Читать далее Всего голосов 15: ↑13 и ↓2 +13 Добавить в закладки60 Комментарии 11 Назад Сюда 1 23 ... 184185Туда Назад Работа Data Scientist 55 вакансий Все вакансии Ваш аккаунт Войти Регистрация Разделы Статьи Новости Хабы Компании Авторы Песочница Информация Устройство сайта Для авторов Для компаний Документы Соглашение Конфиденциальность Услуги Корпоративный блог Медийная реклама Нативные проекты Образовательные программы Стартапам Facebook Twitter VK Telegram Youtube Яндекс Дзен Настройка языкаТехническая поддержка © 2006–2025, Habr